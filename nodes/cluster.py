import json
import jieba
import numpy as np
from langchain_core.messages import AIMessage
from langchain_openai import ChatOpenAI
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from transformers import pipeline


def _sentiment_analysis(texts: list[str]) -> dict:
    """ä½¿ç”¨ transformers é€²è¡Œä¸­æ–‡æƒ…æ„Ÿåˆ†æã€‚"""
    try:
        classifier = pipeline(
            "sentiment-analysis",
            model="uer/roberta-base-finetuned-jd-binary-chinese",
            truncation=True,
            max_length=512,
        )
    except Exception:
        # å¦‚æœæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨åŸºæœ¬çš„æ›¿ä»£æ–¹æ¡ˆ
        classifier = None

    positive = 0
    negative = 0
    neutral = 0
    details = []

    for text in texts:
        if classifier:
            try:
                result = classifier(text[:512])[0]
                label = result["label"]
                score = result["score"]

                if label == "positive" or label == "LABEL_1":
                    if score > 0.6:
                        sentiment = "æ­£é¢"
                        positive += 1
                    else:
                        sentiment = "ä¸­æ€§"
                        neutral += 1
                else:
                    if score > 0.6:
                        sentiment = "è² é¢"
                        negative += 1
                    else:
                        sentiment = "ä¸­æ€§"
                        neutral += 1

                details.append({"sentiment": sentiment, "score": round(score, 4)})
            except Exception:
                details.append({"sentiment": "ä¸­æ€§", "score": 0.5})
                neutral += 1
        else:
            details.append({"sentiment": "ä¸­æ€§", "score": 0.5})
            neutral += 1

    return {
        "æ­£é¢": positive,
        "è² é¢": negative,
        "ä¸­æ€§": neutral,
        "details": details,
    }


def _cluster_texts(texts: list[str], n_clusters: int = 5) -> tuple[list[int], np.ndarray]:
    """ä½¿ç”¨ sentence-transformers + KMeans é€²è¡Œæ–‡å­—åˆ†ç¾¤ã€‚"""
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    embeddings = model.encode(texts, show_progress_bar=False)

    # ç¢ºä¿åˆ†ç¾¤æ•¸ä¸è¶…éè³‡æ–™æ•¸
    actual_k = min(n_clusters, len(texts))
    if actual_k < 2:
        return [0] * len(texts), embeddings

    kmeans = KMeans(n_clusters=actual_k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(embeddings)
    return labels.tolist(), embeddings


def _name_clusters(cluster_items: dict[int, list[str]]) -> dict[str, str]:
    """ä½¿ç”¨ LLM ç‚ºæ¯å€‹åˆ†ç¾¤å‘½åã€‚"""
    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    cluster_labels = {}
    for cluster_id, items in cluster_items.items():
        sample = items[:10]  # å–æœ€å¤š 10 å‰‡ä½œç‚ºç¯„ä¾‹
        sample_text = "\n".join(f"- {item}" for item in sample)

        response = llm.invoke([
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å®¢æœå›é¥‹åˆ†æå°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹å›é¥‹å…§å®¹ï¼Œçµ¦é€™å€‹åˆ†ç¾¤ä¸€å€‹ç°¡çŸ­çš„ä¸­æ–‡æ¨™ç±¤åç¨±ï¼ˆ5å€‹å­—ä»¥å…§ï¼‰ã€‚åªå›å‚³æ¨™ç±¤åç¨±ã€‚"},
            {"role": "user", "content": f"å›é¥‹å…§å®¹ï¼š\n{sample_text}"},
        ])
        cluster_labels[str(cluster_id)] = response.content.strip()

    return cluster_labels


def _build_knowledge_map(cluster_labels: dict, cluster_items: dict) -> dict:
    """ä½¿ç”¨ LLM å»ºç«‹éšå±¤æ¨¹ç‹€ Knowledge Map è³‡æ–™çµæ§‹ã€‚"""
    llm = ChatOpenAI(model="gpt-4.1", temperature=0)

    # æº–å‚™åˆ†ç¾¤æ‘˜è¦
    cluster_summary = ""
    for cid, label in cluster_labels.items():
        items = cluster_items.get(int(cid), [])[:5]
        items_text = "ã€".join(items)
        cluster_summary += f"\nåˆ†ç¾¤ã€Œ{label}ã€ï¼š{items_text}"

    prompt = f"""æ ¹æ“šä»¥ä¸‹å®¢æœå›é¥‹åˆ†ç¾¤çµæœï¼Œå»ºç«‹ä¸€å€‹ä¸‰å±¤éšå±¤çš„ Knowledge Mapï¼š
å¤§åˆ†é¡ â†’ å°åˆ†é¡ â†’ é—œéµå­—

åˆ†ç¾¤è³‡æ–™ï¼š
{cluster_summary}

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼Œçµæ§‹å¦‚ä¸‹ï¼š
{{
    "name": "é¡§å®¢å›é¥‹",
    "children": [
        {{
            "name": "å¤§åˆ†é¡åç¨±",
            "children": [
                {{"name": "å°åˆ†é¡åç¨±", "keywords": ["é—œéµå­—1", "é—œéµå­—2"]}},
                ...
            ]
        }},
        ...
    ]
}}

åªå›å‚³ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

    response = llm.invoke([
        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹è³‡æ–™åˆ†æå°ˆå®¶ï¼Œæ“…é•·å°‡å®¢æœå›é¥‹è³‡æ–™çµæ§‹åŒ–ã€‚"},
        {"role": "user", "content": prompt},
    ])

    try:
        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        return json.loads(content.strip())
    except (json.JSONDecodeError, IndexError):
        # å›é€€æ–¹æ¡ˆï¼šç›´æ¥å¾åˆ†ç¾¤å»ºç«‹ç°¡å–®çµæ§‹
        children = []
        for cid, label in cluster_labels.items():
            items = cluster_items.get(int(cid), [])
            # ä½¿ç”¨ jieba æ“·å–é—œéµå­—
            all_words = []
            for item in items:
                words = [w for w in jieba.cut(item) if len(w) > 1]
                all_words.extend(words)
            # å–å‰ 5 å€‹é«˜é »å­—
            from collections import Counter
            top_words = [w for w, _ in Counter(all_words).most_common(5)]
            children.append({
                "name": label,
                "children": [{"name": label, "keywords": top_words}],
            })
        return {"name": "é¡§å®¢å›é¥‹", "children": children}


def cluster_node(state: dict) -> dict:
    """Cluster ç¯€é»ï¼šæ„è¦‹åˆ†ç¾¤ + æƒ…æ„Ÿåˆ†æã€‚"""
    raw_data = state.get("raw_data")

    if not raw_data:
        return {
            "messages": [AIMessage(content="âŒ å°šç„¡åŸå§‹è³‡æ–™ï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™è®€å–ã€‚")],
        }

    # æ“·å–å›é¥‹å…§å®¹
    texts = [row.get("å›é¥‹å…§å®¹", "") for row in raw_data if row.get("å›é¥‹å…§å®¹")]
    feedback_ids = [row.get("å›é¥‹ç·¨è™Ÿ", f"FB{i}") for i, row in enumerate(raw_data) if row.get("å›é¥‹å…§å®¹")]

    if not texts:
        return {
            "messages": [AIMessage(content="âŒ è³‡æ–™ä¸­æ²’æœ‰å›é¥‹å…§å®¹å¯åˆ†æã€‚")],
        }

    # æƒ…æ„Ÿåˆ†æ
    sentiment_result = _sentiment_analysis(texts)
    for i, detail in enumerate(sentiment_result["details"]):
        if i < len(feedback_ids):
            detail["å›é¥‹ç·¨è™Ÿ"] = feedback_ids[i]

    # åˆ†ç¾¤
    n_clusters = 5
    labels, _ = _cluster_texts(texts, n_clusters)

    # çµ„ç¹”åˆ†ç¾¤é …ç›®
    cluster_items: dict[int, list[str]] = {}
    for i, (text, label) in enumerate(zip(texts, labels)):
        cluster_items.setdefault(label, []).append(text)

    # å‘½ååˆ†ç¾¤
    cluster_labels = _name_clusters(cluster_items)

    # çµ„è£åˆ†ç¾¤çµæœ
    items_list = []
    for i, (text, label) in enumerate(zip(texts, labels)):
        items_list.append({
            "å›é¥‹ç·¨è™Ÿ": feedback_ids[i] if i < len(feedback_ids) else f"FB{i}",
            "cluster_id": label,
            "content": text,
        })

    clusters = {
        "n_clusters": len(cluster_items),
        "cluster_labels": cluster_labels,
        "items": items_list,
    }

    # å»ºç«‹ Knowledge Map è³‡æ–™
    knowledge_map_data = _build_knowledge_map(cluster_labels, cluster_items)

    # çµ„è£æ‘˜è¦è¨Šæ¯
    cluster_summary = "\n".join(
        f"  - {cluster_labels.get(str(cid), f'ç¾¤çµ„{cid}')}ï¼š{len(items)} ç­†"
        for cid, items in cluster_items.items()
    )
    sentiment_summary = f"æ­£é¢ {sentiment_result['æ­£é¢']} ç­†ã€è² é¢ {sentiment_result['è² é¢']} ç­†ã€ä¸­æ€§ {sentiment_result['ä¸­æ€§']} ç­†"

    msg = f"ğŸ” åˆ†æå®Œæˆï¼š\n\nğŸ“Š æƒ…æ„Ÿåˆ†æï¼š{sentiment_summary}\n\nğŸ“‹ åˆ†ç¾¤çµæœï¼ˆå…± {len(cluster_items)} ç¾¤ï¼‰ï¼š\n{cluster_summary}"

    return {
        "sentiment_result": sentiment_result,
        "clusters": clusters,
        "knowledge_map_data": knowledge_map_data,
        "messages": [AIMessage(content=msg)],
    }
